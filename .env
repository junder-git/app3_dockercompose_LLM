# .env file - Devstral AI Chat Application
# CONSERVATIVE settings for RTX 3060 Ti (8GB VRAM) + 32GB RAM

# Security
SECRET_KEY=your-very-secret-key-change-this-in-production-make-it-long-and-random
SECURE_COOKIES=false  # Set to true for HTTPS production

# Admin User Configuration
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin123

# Database Configuration
REDIS_URL=redis://redis:6379/0

# AI Model Configuration - Devstral:24b CONSERVATIVE
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=devstral:24b

# Model Generation Parameters - CONSERVATIVE for memory constraints
MODEL_TEMPERATURE=0.7
MODEL_TOP_P=0.9
MODEL_MAX_TOKENS=2048           # Reduced from 4096 to save memory
MODEL_TIMEOUT=180               # Reduced to 3 minutes

# Rate Limiting - More conservative for large model
RATE_LIMIT_MESSAGES_PER_MINUTE=5     # Reduced from 8
RATE_LIMIT_LOGIN_ATTEMPTS_PER_MINUTE=3
RATE_LIMIT_API_CALLS_PER_MINUTE=15   # Reduced from 25

# Chat Configuration
CHAT_CACHE_TTL_SECONDS=7200     # 2 hours (longer cache for expensive responses)
CHAT_HISTORY_LIMIT=30           # Reduced from 50 to save memory
SESSION_LIFETIME_DAYS=7

# Application Configuration
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=2                   # Keep at 2 workers

# Docker Resource Limits - CONSERVATIVE for Devstral:24b
QUART_APP_MEMORY_LIMIT=3G       # Reduced from 4G
QUART_APP_MEMORY_RESERVATION=1G # Reduced from 2G
OLLAMA_MEMORY_LIMIT=24G         # Reduced from 26G (leave 8GB for system)
OLLAMA_MEMORY_RESERVATION=12G   # Reduced from 16G
REDIS_MEMORY_LIMIT=1G           # Reduced from 2G

# GPU Configuration - CONSERVATIVE hybrid CPU+GPU for 24B model
NVIDIA_VISIBLE_DEVICES=0
CUDA_MEMORY_FRACTION=0.90       # Slightly reduced from 0.95
OLLAMA_GPU_LAYERS=12            # Reduced from 15 (more conservative GPU usage)
OLLAMA_NUM_THREAD=10            # Increased from 8 (more CPU compensation)
OLLAMA_LOW_VRAM=1               # Enable low VRAM optimizations

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# SSE Configuration - Event-driven updates (more conservative)
SSE_HEARTBEAT_INTERVAL=30
SSE_MAX_CONNECTIONS=15          # Reduced from 25
SSE_RETRY_TIMEOUT=5000

# Performance Optimizations for 24B model (CONSERVATIVE)
OLLAMA_FLASH_ATTENTION=1        # Enable flash attention
OLLAMA_ROPE_SCALING=1           # Better context handling
OLLAMA_MAIN_GPU=0               # Primary GPU designation
OLLAMA_CONTEXT_SIZE=16384       # Reduced context size (was 32768)
OLLAMA_BATCH_SIZE=128           # Smaller batch size (was 256)