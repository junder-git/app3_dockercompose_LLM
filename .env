# OPTIMIZED .env - Configuration for Ollama with GGUF Model
# =============================================================================

# üîê JWT CONFIGURATION
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production-min-32-chars

# Domain Configuration
ALLOWED_DOMAINS=localhost,127.0.0.1,ai.junder.uk

# Admin User Configuration
ADMIN_USERNAME=admin1
ADMIN_PASSWORD=admin1
ADMIN_USER_ID=admin

# =============================================================================
# ü§ñ MODEL CONFIGURATION - OLLAMA OPTIMIZED
# =============================================================================
MODEL_URL=http://ollama:11434
MODEL_NAME=devstral
MODEL_GGUF_PATH=/root/.ollama/models/Devstral-Small-2507-Q4_K_M.gguf

# =============================================================================
# üéõÔ∏è MODEL PARAMETERS - OPTIMIZED FOR DEVSTRAL SMALL
# =============================================================================

# Core sampling parameters
MODEL_TEMPERATURE=0.7          # Good balance for code generation
MODEL_TOP_P=0.9               # Standard value, works well
MODEL_TOP_K=40                # Standard value, good diversity
MODEL_MIN_P=0.05              # Better than 0.0, helps with quality
MODEL_REPEAT_PENALTY=1.1      # Good default
MODEL_REPEAT_LAST_N=64        # Reasonable lookback

# Context and prediction settings - OPTIMIZED
MODEL_NUM_CTX=2048            # Increased from 512 - you have 22GB RAM!
MODEL_NUM_PREDICT=512         # Increased from 256 - allow longer responses
MODEL_SEED=0                  # 0 = random seed each time

# Hardware optimization - CORRECTED
OLLAMA_NUM_THREAD=6           # Matches your CPU allocation
OLLAMA_MMAP=true              # Changed to true - better memory efficiency
OLLAMA_NUM_PARALLEL=2         # Reduced from 3 - more stable with limited context
OLLAMA_GPU_LAYERS=20          # Conservative for 8GB VRAM - test and increase if stable

# Ollama server configuration - OPTIMIZED
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_ORIGINS=*
OLLAMA_MODELS=/root/.ollama/models
OLLAMA_MAX_LOADED_MODELS=1    # Keep at 1 for resource efficiency
OLLAMA_FLASH_ATTENTION=true   # Changed to true - boolean, not 1

# Ollama initialization script settings
OLLAMA_MAX_RETRIES=30         # Reduced from 45 - faster startup
OLLAMA_RETRY_INTERVAL=15      # Reduced from 20 - faster startup
MODELFILE_PATH=/root/.ollama/scripts/Modelfile

# Ollama runtime settings
OLLAMA_KEEP_ALIVE=999h        # Keep model in memory for 999 hours
OLLAMA_USE_MMAP=false         # Disable memory mapping for better performance

# =============================================================================
# üì± APPLICATION SETTINGS
# =============================================================================
RATE_LIMIT_MESSAGES_PER_MINUTE=12
MAX_MESSAGE_LENGTH=8000

# User Management
MAX_CHATS_PER_USER=1
MAX_PENDING_USERS=5
MIN_USERNAME_LENGTH=3
MAX_USERNAME_LENGTH=12
MIN_PASSWORD_LENGTH=6
MAX_PASSWORD_LENGTH=16

# =============================================================================
# üê≥ DOCKER RESOURCES - OPTIMIZED FOR CPU+GPU HYBRID
# =============================================================================

# Hardware configuration
OLLAMA_CPU=6.0
OLLAMA_MEMORY=22G
REDIS_MEMORY=1G
REDIS_CPU=0.5
NGINX_MEMORY=1G
NGINX_CPU=0.5

# =============================================================================
# üì¶ REDIS & NETWORK
# =============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_URL=redis://redis:6379/0

# =============================================================================
# üìù LOGGING
# =============================================================================
LOG_LEVEL=INFO
LOG_FORMAT=json