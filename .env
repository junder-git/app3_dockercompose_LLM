# .env file 
# Compose general resource limits
QUART_APP_MEMORY=1G
QUART_APP_CPU=1
OLLAMA_MEMORY=16G
OLLAMA_CPU=4
REDIS_MEMORY=1G
REDIS_CPU=1
NGINX_MEMORY=1G
NGINX_CPU=1


# Ollama - PERMANENT RAM/VRAM LOADING 7.5GB mode
OLLAMA_HOST=0.0.0.0
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=deepseek-r1:1.5b       #  devstral:24b  ~14Gb  ||  deepseek-r1:1.5b  ~1.5Gb
NVIDIA_VISIBLE_DEVICES=0
CUDA_MEMORY_FRACTION=0.96           # Use almost all VRAM
OLLAMA_GPU_LAYERS=12                # Target
OLLAMA_NUM_THREAD=4                 # Fewer CPU threads (more on GPU)
OLLAMA_MLOCK=true                   # Lock model in RAM/VRAM - prevents swapping
OLLAMA_MMAP=false                   # Disable memory mapping - force full RAM loading
OLLAMA_NUMA=false                   # Disable NUMA for single-GPU setups
LOG_LEVEL=INFO
LOG_FORMAT=json
OLLAMA_MAIN_GPU=0
OLLAMA_CONTEXT_SIZE=16384           # Large context
OLLAMA_BATCH_SIZE=256               # Large batches
OLLAMA_KEEP_ALIVE=-1                # Permanent loading
OLLAMA_MAX_LOADED_MODELS=1          # Only load one model at a time
OLLAMA_LOAD_TIMEOUT=15m             # Allow time for full loading
OLLAMA_NOPRUNE=true                 # Never unload models automatically
# REMOVED INVALID OPTIONS:
# OLLAMA_FLASH_ATTENTION (not valid)
# OLLAMA_LOW_VRAM (not valid)
# OLLAMA_ROPE_SCALING (not valid)

# Quart web app
SECRET_KEY=your-very-secret-key-change-this-in-production
SECURE_COOKIES=false
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin123
REDIS_URL=redis://redis:6379/0
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=1
CHAT_CACHE_TTL_SECONDS=7200
CHAT_HISTORY_LIMIT=25               # Good context with 7.5GB
SESSION_LIFETIME_DAYS=7
SSE_HEARTBEAT_INTERVAL=30
SSE_MAX_CONNECTIONS=50              # Much higher for better performance
SSE_RETRY_TIMEOUT=5000
MODEL_TEMPERATURE=0.7
MODEL_TOP_P=0.9
MODEL_MAX_TOKENS=2048               # High performance - longer responses
MODEL_TIMEOUT=120                   # 2 minutes (model is pre-loaded)



