# .env file - Devstral AI Chat Application
# Optimized for RTX 3060 Ti (8GB VRAM) + 32GB RAM

# Security
SECRET_KEY=your-very-secret-key-change-this-in-production-make-it-long-and-random
SECURE_COOKIES=false  # Set to true for HTTPS production

# Admin User Configuration
ADMIN_USERNAME=admin
ADMIN_PASSWORD=admin123

# Database Configuration
REDIS_URL=redis://redis:6379/0

# AI Model Configuration - Devstral:24b
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=devstral:24b

# Model Generation Parameters - Optimized for 24B hybrid setup
MODEL_TEMPERATURE=0.7
MODEL_TOP_P=0.9
MODEL_MAX_TOKENS=4096           # Increased for better code generation
MODEL_TIMEOUT=300               # 5 minutes for complex responses

# Rate Limiting - Reduced for larger model
RATE_LIMIT_MESSAGES_PER_MINUTE=8
RATE_LIMIT_LOGIN_ATTEMPTS_PER_MINUTE=5
RATE_LIMIT_API_CALLS_PER_MINUTE=25

# Chat Configuration
CHAT_CACHE_TTL_SECONDS=7200     # 2 hours (longer cache for expensive responses)
CHAT_HISTORY_LIMIT=50           # Reduced context to save memory
SESSION_LIFETIME_DAYS=7

# Application Configuration
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=2                   # Reduced workers to save memory for Ollama

# Docker Resource Limits - Optimized for Devstral:24b
QUART_APP_MEMORY_LIMIT=4G
QUART_APP_MEMORY_RESERVATION=2G
OLLAMA_MEMORY_LIMIT=26G         # Most RAM for model
OLLAMA_MEMORY_RESERVATION=16G
REDIS_MEMORY_LIMIT=2G

# GPU Configuration - Hybrid CPU+GPU for 24B model
NVIDIA_VISIBLE_DEVICES=0
CUDA_MEMORY_FRACTION=0.95       # Use almost all 8GB VRAM
OLLAMA_GPU_LAYERS=15            # ~50% layers on GPU (fits in 8GB)
OLLAMA_NUM_THREAD=8             # Use 8 CPU cores for remaining layers
OLLAMA_LOW_VRAM=1               # Enable low VRAM optimizations

# Logging
LOG_LEVEL=INFO
LOG_FORMAT=json

# SSE Configuration - Event-driven updates
SSE_HEARTBEAT_INTERVAL=30
SSE_MAX_CONNECTIONS=25          # Fewer connections due to memory usage
SSE_RETRY_TIMEOUT=5000

# Performance Optimizations for 24B model
OLLAMA_FLASH_ATTENTION=1        # Enable flash attention
OLLAMA_ROPE_SCALING=1           # Better context handling
OLLAMA_MAIN_GPU=0               # Primary GPU designation