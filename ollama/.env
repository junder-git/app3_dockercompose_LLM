# ollama/.env - Ollama Service Configuration
# ===== OLLAMA CORE SETTINGS =====
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_NOHISTORY=false

# ===== GPU CONFIGURATION =====
NVIDIA_VISIBLE_DEVICES=all
CUDA_VISIBLE_DEVICES=0
CUDA_DEVICE_ORDER=PCI_BUS_ID
MAX_VRAM=7GB

# ===== OLLAMA PERFORMANCE SETTINGS =====
# Concurrent Processing
OLLAMA_NUM_PARALLEL=4
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_MAX_QUEUE=20

# Model Loading and Retention
OLLAMA_KEEP_ALIVE=-1 #perma in ram
OLLAMA_LOAD_TIMEOUT=600
OLLAMA_REQUEST_TIMEOUT=300

# ===== ADVANCED PERFORMANCE SETTINGS =====
# Flash Attention - Reduces VRAM usage and improves speed
OLLAMA_FLASH_ATTENTION=1

# Context Configuration - Optimized for 8GB VRAM
OLLAMA_CONTEXT_SIZE=4096
OLLAMA_BATCH_SIZE=2048

# GPU Memory Optimizations
OLLAMA_LOW_VRAM=false
OLLAMA_GPU_LAYERS=-1
OLLAMA_MAIN_GPU=0
OLLAMA_TENSOR_SPLIT=1.0

# Model Loading Strategy
OLLAMA_PRELOAD_MODEL=true
OLLAMA_OFFLOAD_KQV=true

# Performance Directories
OLLAMA_RUNNERS_DIR=/tmp/ollama_runners
OLLAMA_TMPDIR=/tmp/ollama_tmp

# ===== CPU OPTIMIZATION =====
# Threading - Utilize available CPU cores
OMP_NUM_THREADS=8
GOMP_CPU_AFFINITY=0-9
MKL_NUM_THREADS=8
OPENBLAS_NUM_THREADS=8
VECLIB_MAXIMUM_THREADS=8

# ===== MEMORY MANAGEMENT =====
MALLOC_ARENA_MAX=1
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,garbage_collection_threshold:0.6