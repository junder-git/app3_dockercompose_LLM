# .env - Complete High Performance Configuration for Open WebUI + Ollama
# 10-core/32GB/8GB VRAM System Optimized for 24B parameter models

# ===== SECURITY =====
WEBUI_SECRET_KEY=your-very-secret-key-change-this-in-production-make-it-long-and-random

# ===== GPU CONFIGURATION =====
NVIDIA_VISIBLE_DEVICES=all
CUDA_VISIBLE_DEVICES=0
CUDA_DEVICE_ORDER=PCI_BUS_ID
MAX_VRAM=7GB                               # Reserve 1GB VRAM for system overhead

# ===== AUTHENTICATION & USER MANAGEMENT =====
WEBUI_AUTH=true
DEFAULT_USER_ROLE=user
ENABLE_SIGNUP=true
ENABLE_LOGIN_FORM=true

# Default admin user configuration
DEFAULT_USER_EMAIL=admin@example.com
DEFAULT_USER_NAME=Admin
DEFAULT_USER_PASSWORD=change-this-password

# ===== OPEN WEBUI CONFIGURATION =====
WEBUI_URL=https://ai.junder.uk
CORS_ALLOW_ORIGIN=https://ai.junder.uk

# User Agent and Warnings
USER_AGENT=Open-WebUI/1.0 (ai.junder.uk)
LANGCHAIN_TRACING_V2=false

# ===== RESOURCE LIMITS =====
# Memory Management - Optimized for 32GB RAM
OLLAMA_MEMORY_LIMIT=24G                    # Use 24GB of 32GB RAM
OLLAMA_MEMORY_RESERVATION=12G              # Reserve 12GB minimum

# ===== STORAGE OPTIMIZATION =====
# Set to SSD path for better I/O performance (optional)
OLLAMA_DATA_PATH=/mnt/nvme/ollama_data
WEBUI_DATA_PATH=/mnt/nvme/webui_data

# ===== OLLAMA CORE SETTINGS =====
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_NOHISTORY=false

# ===== OLLAMA PERFORMANCE SETTINGS =====
# Concurrent Processing
OLLAMA_NUM_PARALLEL=4                      # Handle 4 requests simultaneously
OLLAMA_MAX_LOADED_MODELS=1                 # Keep 1 model in memory
OLLAMA_MAX_QUEUE=20                        # Queue depth for incoming requests

# Model Loading and Retention
OLLAMA_KEEP_ALIVE=-1                      # Keep models loaded for 30 minutes
OLLAMA_LOAD_TIMEOUT=600                    # 10 minutes timeout for large models
OLLAMA_REQUEST_TIMEOUT=300                 # 5 minutes per request

# ===== ADVANCED PERFORMANCE SETTINGS =====
# Flash Attention - Reduces VRAM usage and improves speed
OLLAMA_FLASH_ATTENTION=1                   # Enable flash attention for better memory efficiency

# Context Configuration - Optimized for 8GB VRAM
OLLAMA_CONTEXT_SIZE=4096                   # 8K context (optimal for 7GB available VRAM)
OLLAMA_BATCH_SIZE=2048                     # Batch processing size

# GPU Memory Optimizations
OLLAMA_LOW_VRAM=false                      # Set to true if you encounter VRAM issues
OLLAMA_GPU_LAYERS=-1                       # Load all layers to GPU (-1 = auto)
OLLAMA_MAIN_GPU=0                          # Use first GPU
OLLAMA_TENSOR_SPLIT=1.0                    # Use 100% of GPU 0

# Model Loading Strategy
OLLAMA_PRELOAD_MODEL=true                  # Preload model for faster inference
OLLAMA_OFFLOAD_KQV=true                    # Offload KV cache to system RAM if needed

# Performance Directories
OLLAMA_RUNNERS_DIR=/tmp/ollama_runners
OLLAMA_TMPDIR=/tmp/ollama_tmp

# ===== CPU OPTIMIZATION =====
# Threading - Utilize all 10 CPU cores
OMP_NUM_THREADS=10                         # OpenMP threads
GOMP_CPU_AFFINITY=0-9                      # Pin threads to all cores 0-9
MKL_NUM_THREADS=10                          # Intel MKL threads
OPENBLAS_NUM_THREADS=10                     # OpenBLAS threads
VECLIB_MAXIMUM_THREADS=10                   # Apple Accelerate threads

# ===== MEMORY MANAGEMENT =====
MALLOC_ARENA_MAX=2                         # Reduce memory fragmentation
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024,garbage_collection_threshold:0.6