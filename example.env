# .env - High Performance Configuration for Open WebUI + Ollama
# 10-core/32GB/8GB VRAM System Optimized for 24B parameter models

# ===== SECURITY =====
SECRET_KEY=your-very-secret-key-change-this-in-production-make-it-long-and-random

# ===== GPU CONFIGURATION =====
NVIDIA_VISIBLE_DEVICES=all
MAX_VRAM=7GB                               # Reserve 1GB VRAM for system overhead

# ===== OPEN WEBUI CONFIGURATION =====
ENABLE_SIGNUP=true
WEBUI_URL=https://ai.junder.uk

# ===== DEFAULT ADMIN USER =====
DEFAULT_USER_EMAIL=j@junder.uk
DEFAULT_USER_NAME=jai
DEFAULT_USER_PASSWORD=admin123

# ===== RESOURCE LIMITS =====
# Memory Management - Optimized for 32GB RAM
OLLAMA_MEMORY_LIMIT=28G                    # Use 28GB of 32GB RAM
OLLAMA_MEMORY_RESERVATION=20G              # Reserve 20GB minimum

# ===== STORAGE OPTIMIZATION =====
# Set to SSD path for better I/O performance (optional)
OLLAMA_DATA_PATH=/mnt/nvme/ollama_data

# ===== OLLAMA PERFORMANCE SETTINGS =====
# Concurrent Processing
OLLAMA_NUM_PARALLEL=4                      # Handle 4 requests simultaneously
OLLAMA_MAX_LOADED_MODELS=1                 # Keep 1 model in memory
OLLAMA_MAX_QUEUE=20                        # Queue depth for incoming requests

# Model Loading and Retention
OLLAMA_KEEP_ALIVE=30m                      # Keep models loaded for 30 minutes
OLLAMA_LOAD_TIMEOUT=600                    # 10 minutes timeout for large models
OLLAMA_REQUEST_TIMEOUT=300                 # 5 minutes per request

# ===== CPU OPTIMIZATION =====
# Threading - Utilize all 10 CPU cores
OMP_NUM_THREADS=10                         # OpenMP threads
GOMP_CPU_AFFINITY=0-9                      # Pin threads to all cores 0-9

# ===== MEMORY MANAGEMENT =====
MALLOC_ARENA_MAX=2                         # Reduce memory fragmentation

# ===== ADVANCED PERFORMANCE SETTINGS =====
# Flash Attention - Reduces VRAM usage and improves speed
OLLAMA_FLASH_ATTENTION=1                   # Enable flash attention for better memory efficiency

# Context Configuration - Optimized for 8GB VRAM
OLLAMA_CONTEXT_SIZE=8192                   # 8K context (optimal for 7GB available VRAM)
OLLAMA_BATCH_SIZE=512                      # Batch processing size

# Additional Memory Optimizations
OLLAMA_LOW_VRAM=false                      # Set to true if you encounter VRAM issues
OLLAMA_GPU_LAYERS=-1                       # Load all layers to GPU (-1 = auto)
OLLAMA_MAIN_GPU=0                          # Use first GPU
OLLAMA_TENSOR_SPLIT=1.0                    # Use 100% of GPU 0

# Model Loading Strategy
OLLAMA_PRELOAD_MODEL=true                  # Preload model for faster inference
OLLAMA_OFFLOAD_KQV=true                    # Offload KV cache to system RAM if needed