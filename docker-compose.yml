services:
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
      args:
        OLLAMA_VERSION: ${OLLAMA_VERSION}
    container_name: devstral-ollama
    privileged: true
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_VERSION=${OLLAMA_VERSION}
      - OLLAMA_HOST=${OLLAMA_HOST}
      - OLLAMA_MODELS=/home/ollama/.ollama/models
      - OLLAMA_MLOCK=${OLLAMA_MLOCK}
      - OLLAMA_MMAP=${OLLAMA_MMAP}
      - KEEP_ALIVE=${KEEP_ALIVE}
      - NOPRUNE=${NOPRUNE}
      - MAX_LOADED_MODELS=${MAX_LOADED_MODELS}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS}
      - OLLAMA_NUM_THREAD=${OLLAMA_NUM_THREAD}
      - OLLAMA_CONTEXT_SIZE=${OLLAMA_CONTEXT_SIZE}
      - OLLAMA_BATCH_SIZE=${OLLAMA_BATCH_SIZE}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - MODEL_DISPLAY_NAME=${MODEL_DISPLAY_NAME}
      - MODEL_DESCRIPTION=${MODEL_DESCRIPTION}
      - MODEL_TEMPERATURE=${MODEL_TEMPERATURE}
      - MODEL_TOP_P=${MODEL_TOP_P}
      - MODEL_TOP_K=${MODEL_TOP_K}
      - MODEL_REPEAT_PENALTY=${MODEL_REPEAT_PENALTY}
      - MODEL_MIROSTAT=${MODEL_MIROSTAT}
      - MODEL_MIROSTAT_ETA=${MODEL_MIROSTAT_ETA}
      - MODEL_MIROSTAT_TAU=${MODEL_MIROSTAT_TAU}
      - MODEL_USE_MMAP=${MODEL_USE_MMAP}
      - MODEL_USE_MLOCK=${MODEL_USE_MLOCK}
      - MODEL_STOP_SEQUENCES=${MODEL_STOP_SEQUENCES}
      - MODEL_MAX_TOKENS=${MODEL_MAX_TOKENS}
      - MODEL_TIMEOUT=${MODEL_TIMEOUT}
      - CHAT_HISTORY_LIMIT=${CHAT_HISTORY_LIMIT}
      - UNLIMITED_TIMEOUT=${UNLIMITED_TIMEOUT}
      - STREAMING_TIMEOUT=${STREAMING_TIMEOUT}
    volumes:
      - ./volumes/ollama:/home/ollama/.ollama
      - ./volumes/ollama-cache:/ollama-cache  # Add binary cache volume
    deploy:
      resources:
        limits:
          cpus: ${OLLAMA_CPU}
          memory: ${OLLAMA_MEMORY}
        reservations:
          cpus: ${OLLAMA_CPU}
          memory: ${OLLAMA_MEMORY}
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - devstral-network

  redis:
    build: ./redis
    container_name: devstral-redis
    ports:
      - "6379:6379"
    environment:
      - ADMIN_USERNAME=${ADMIN_USERNAME}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - ADMIN_USER_ID=${ADMIN_USER_ID}
      - JWT_SECRET=${JWT_SECRET}
    volumes:
      - ./volumes/redis_data:/data
      - ./volumes/redis_logs:/var/log/redis
    networks:
      - devstral-network
    deploy:
      resources:
        limits:
          cpus: ${REDIS_CPU}
          memory: ${REDIS_MEMORY}
        reservations:
          cpus: ${REDIS_CPU}
          memory: ${REDIS_MEMORY}
    restart: unless-stopped
    healthcheck:
      test: [
        "CMD-SHELL",
        "redis-cli EXISTS user:${ADMIN_USERNAME} || exit 1"
      ]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 10s

  nginx:
    build: ./nginx
    container_name: devstral-nginx
    ports:
      - "80:80"
    environment:
      - JWT_SECRET=${JWT_SECRET}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - OLLAMA_URL=${OLLAMA_URL}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - MIN_PASSWORD_LENGTH=${MIN_PASSWORD_LENGTH}
    volumes:
      - ./volumes/nginx_cache:/var/cache/nginx
      - ./volumes/nginx_logs:/var/log/nginx
    depends_on:
      ollama:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - devstral-network
    deploy:
      resources:
        limits:
          cpus: ${NGINX_CPU}
          memory: ${NGINX_MEMORY}
        reservations:
          cpus: ${NGINX_CPU}
          memory: ${NGINX_MEMORY}
    restart: unless-stopped

networks:
  devstral-network:
    driver: bridge