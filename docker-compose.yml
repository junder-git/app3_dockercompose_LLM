services:
  ollama:
    build:
      context: ./ollama
      dockerfile: Dockerfile
    container_name: devstral-ollama
    privileged: true
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST}
      - OLLAMA_MODELS=/home/ollama/.ollama/models
      - OLLAMA_MLOCK=${OLLAMA_MLOCK}
      - OLLAMA_MMAP=${OLLAMA_MMAP}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE}
      - OLLAMA_NOPRUNE=${OLLAMA_NOPRUNE}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS}
      - OLLAMA_NUM_THREAD=${OLLAMA_NUM_THREAD}
      - OLLAMA_CONTEXT_SIZE=${OLLAMA_CONTEXT_SIZE}
      - OLLAMA_BATCH_SIZE=${OLLAMA_BATCH_SIZE}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - MODEL_DISPLAY_NAME=${MODEL_DISPLAY_NAME}
      - MODEL_DESCRIPTION=${MODEL_DESCRIPTION}
      - MODEL_TEMPERATURE=${MODEL_TEMPERATURE}
      - MODEL_TOP_P=${MODEL_TOP_P}
      - MODEL_TOP_K=${MODEL_TOP_K}
      - MODEL_REPEAT_PENALTY=${MODEL_REPEAT_PENALTY}
      - MODEL_MIROSTAT=${MODEL_MIROSTAT}
      - MODEL_MIROSTAT_ETA=${MODEL_MIROSTAT_ETA}
      - MODEL_MIROSTAT_TAU=${MODEL_MIROSTAT_TAU}
      - MODEL_USE_MMAP=${MODEL_USE_MMAP}
      - MODEL_USE_MLOCK=${MODEL_USE_MLOCK}
      - MODEL_STOP_SEQUENCES=${MODEL_STOP_SEQUENCES}
      - MODEL_MAX_TOKENS=${MODEL_MAX_TOKENS}
      - MODEL_TIMEOUT=${MODEL_TIMEOUT}
      - CHAT_HISTORY_LIMIT=${CHAT_HISTORY_LIMIT}
      - UNLIMITED_TIMEOUT=${UNLIMITED_TIMEOUT}
      - STREAMING_TIMEOUT=${STREAMING_TIMEOUT}
    volumes:
      - ./volumes/ollama:/home/ollama/.ollama
    deploy:
      resources:
        limits:
          cpus: ${OLLAMA_CPU}
          memory: ${OLLAMA_MEMORY}
        reservations:
          cpus: ${OLLAMA_CPU}
          memory: ${OLLAMA_MEMORY}
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    networks:
      - devstral-network

  # Redis - Direct access via redis2 module (no HTTP wrapper needed)
  redis:
    build: ./redis
    container_name: devstral-redis
    ports:
      - "6379:6379"  # Expose for debugging/monitoring
    volumes:
      - ./volumes/redis_data:/data
      - ./volumes/redis_logs:/var/log/redis
    networks:
      - devstral-network
    deploy:
      resources:
        limits:
          cpus: ${REDIS_CPU}
          memory: ${REDIS_MEMORY}
        reservations:
          cpus: ${REDIS_CPU}
          memory: ${REDIS_MEMORY}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Nginx with njs and redis2 module
  nginx:
    build: ./nginx
    container_name: devstral-nginx
    ports:
      - "80:80"
    environment:
      - ALLOWED_DOMAINS=${ALLOWED_DOMAINS}
      - ADMIN_USERNAME=${ADMIN_USERNAME}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - MAX_PENDING_USERS=${MAX_PENDING_USERS}
    volumes:
      - ./volumes/nginx_cache:/var/cache/nginx
      - ./volumes/nginx_logs:/var/log/nginx
    depends_on:
      - ollama
      - redis
    networks:
      - devstral-network
    deploy:
      resources:
        limits:
          cpus: ${NGINX_CPU}
          memory: ${NGINX_MEMORY}
        reservations:
          cpus: ${NGINX_CPU}
          memory: ${NGINX_MEMORY}
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  devstral-network:
    driver: bridge